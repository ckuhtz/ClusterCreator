---
- name: Ensure template was created correctly
  hosts: all
  tags:
    - ensure_space
  tasks:
    - name: Find all matching firstboot log files
      find:
        paths: "/var/log"
        patterns: "template-firstboot-*.log"
      register: log_files
    - name: Check each firstboot log file for 'No space left' text
      shell: "grep 'No space left' {{ item.path }}"
      register: grep_output
      failed_when: grep_output.rc == 0
      with_items: "{{ log_files.files }}"
      ignore_errors: true
      no_log: true
    - name: Fail the play if 'No space left' is found in any firstboot log file
      fail:
        msg: "'No space left' found in firstboot log files. Template VM did not have enough space to install all packages. Increase the TEMPLATE_DISK_SIZE and re-create template, then recreate the VM(s)."
      when: grep_output.results | selectattr('rc', 'equalto', 0) | list | count > 0

    - name: Check firstboot log files for failures to ensure all packages were installed correctly (excluding specific patterns)
      shell: |
        grep -iE 'Error|Fatal' {{ item.path }} | grep -vE 'mysql/error.log|errors = false|liberror|No error reported|Failed to open terminal.debconf'
      # ADD EXCLUSIONS HERE^
      register: grep_output
      failed_when: grep_output.rc == 0
      with_items: "{{ log_files.files }}"
      ignore_errors: true
      no_log: true
    - name: Warn and pause if 'Error' or 'Fatal' is found in any firstboot log files, excluding specific instances
      pause:
        prompt: "Warning: 'Error' or 'Fatal' strings found in firstboot log files. Template VM may have an issue with its firstboot scripts. Investigate /var/log/template-firstboot-*.log files and, if needed, recreate the template and the VM(s). You can add exclusions in the prepare-nodes playbook (~ line 26) to filter out false-positives. Press enter to ignore this warning or Ctrl+C to abort."
      when: grep_output.results | selectattr('rc', 'equalto', 0) | list | count > 0

- name: Prepare nodes
  hosts: all
  any_errors_fatal: true
  become: true  # This ensures all tasks are run with elevated privileges
  vars:
    cluster_config: "{{ lookup('file', 'tmp/{{ cluster_name }}/cluster_config.json') | from_json }}"
  tasks:

    - name: Add IPv6 forwarding line to sysctl.d file (if IPv6 is enabled)
      ansible.builtin.lineinfile:
        path: /etc/sysctl.d/k8s_sysctl.conf
        line: 'net.ipv6.conf.all.forwarding        = 1'
        state: present
      when: cluster_config.networking.ipv6.enabled and cluster_config.networking.ipv6.dual_stack
    - name: Reload sysctl to apply changes (if IPv6 is enabled)
      ansible.builtin.command: sysctl --system
      when: cluster_config.networking.ipv6.enabled and cluster_config.networking.ipv6.dual_stack

    - name: Configure kubelet with the correct interface IP
      shell: |
        {% if cluster_config.networking.ipv6.enabled and cluster_config.networking.ipv6.dual_stack %}
        node_ip="{{ ipv4 }},{{ ipv6 }}"
        {% else %}
        node_ip="{{ ipv4 }}"
        {% endif %}

        kubelet_extra_args="--node-ip=${node_ip} --max-pods={{ cluster_config.max_pods_per_node }} --housekeeping-interval=5s"
        # the housekeeping-interval was a fix I needed to fix cadvisor metrics not being deduplicated correctly when sent to Mimir.

        echo "KUBELET_EXTRA_ARGS=\"$kubelet_extra_args\"" | sudo tee /etc/default/kubelet
      args:
        executable: /bin/bash
      tags:
        - kubelet_node_ip

    # needed on recent Ubuntu versions. https://www.reddit.com/r/kubernetes/comments/1dv6z9d/ubuntu_2404_pod_termination_issue/
    - name: Check if /etc/apparmor.d/runc profile exists
      ansible.builtin.stat:
        path: /etc/apparmor.d/runc
      register: apparmor_runc_file_exists
    - name: Check if /etc/apparmor.d/runc profile is disabled
      ansible.builtin.stat:
        path: /etc/apparmor.d/disable/runc
      register: apparmor_runc_file_disabled
    - name: Disable Default AppArmor Profile for Runc (containerd has its own)
      ansible.builtin.command: sudo ln -s /etc/apparmor.d/runc /etc/apparmor.d/disable/
      ignore_errors: yes
      tags:
        - disable_apparmor
      when: apparmor_runc_file_exists.stat.exists and not apparmor_runc_file_disabled.stat.exists
    - name: Reload AppArmor
      ansible.builtin.command: sudo apparmor_parser -R /etc/apparmor.d/runc
      ignore_errors: yes
      tags:
        - disable_apparmor
      when: apparmor_runc_file_exists.stat.exists and not apparmor_runc_file_disabled.stat.exists

    - name: Set correct locale
      shell: |
        echo "LANG=en_US.UTF-8
        LANGUAGE=en_US" > /etc/default/locale
        localectl set-locale LANG=en_US.UTF-8
        touch /var/lib/cloud/instance/locale-check.skip
      args:
        executable: /bin/bash
      tags:
        - set_locale

    - name: Add alias for kubectl
      lineinfile:
        path: "{{ cluster_config.ssh.ssh_home }}/.bashrc"
        create: yes
        line: "alias k='kubectl'"
        insertafter: EOF
      tags:
        - set_aliases
    - name: Add alias for clear
      lineinfile:
        path: "{{ cluster_config.ssh.ssh_home }}/.bashrc"
        create: yes
        line: "alias c='clear'"
        insertafter: EOF
      tags:
        - set_aliases
    - name: Add alias for history
      lineinfile:
        path: "{{ cluster_config.ssh.ssh_home }}/.bashrc"
        create: yes
        line: "alias h='history'"
        insertafter: EOF
      tags:
        - set_aliases

    - name: Check if any scripts exist in /root directory
      stat:
        path: /root/*.sh
      register: scripts_exist

    - name: Move scripts from root directory to user directory (if any exist)
      shell: |
        cp /root/*.sh {{ cluster_config.ssh.ssh_home }}/
        chmod +x {{ cluster_config.ssh.ssh_home }}/*.sh
      args:
        executable: /bin/bash
      become: true
      when: scripts_exist.stat.exists
      tags:
        - copy_scripts

    - name: Add VIP and hostname to /etc/hosts
      ansible.builtin.lineinfile:
        path: /etc/hosts
        line: "{{ cluster_config.networking.kube_vip.vip }} {{ cluster_config.networking.kube_vip.vip_hostname }}"
        create: yes
      tags: etc_hosts
    - name: Add VIP and hostname to cloud templates (if exists)
      ansible.builtin.lineinfile:
        path: /etc/cloud/templates/hosts.debian.tmpl
        line: "{{ cluster_config.networking.kube_vip.vip }} {{ cluster_config.networking.kube_vip.vip_hostname }}"
        create: yes
      when: ansible_facts['os_family'] == "Debian"
      tags: etc_hosts
    - name: Update /etc/hosts with node entries
      ansible.builtin.lineinfile:
        path: /etc/hosts
        line: "{{ hostvars[item].ansible_host }} {{ item }}"
        state: present
      loop: "{{ groups['all'] }}"
      tags:
        - etc_hosts
    - name: Update /etc/hosts with node entries to cloud templates (if exists)
      ansible.builtin.lineinfile:
        path: /etc/cloud/templates/hosts.debian.tmpl
        line: "{{ hostvars[item].ansible_host }} {{ item }}"
        state: present
      loop: "{{ groups['all'] }}"
      when: ansible_facts['os_family'] == "Debian"
      tags:
        - etc_hosts
